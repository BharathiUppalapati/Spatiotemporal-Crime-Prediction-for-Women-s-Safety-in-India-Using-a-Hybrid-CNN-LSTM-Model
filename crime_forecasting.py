# -*- coding: utf-8 -*-
"""Copy of crime_forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ukwAZYfr_D0GSZwxGYDVoLmZT6tAK9hm
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, TimeDistributed
from tensorflow.keras.optimizers import Adam

from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
data_path = '/content/drive/MyDrive/dataset/crimes_against_women_2001-2014.csv'
df = pd.read_csv(data_path)

# Inspect the data
print(df.head())

# Drop the unnecessary 'Unnamed: 0' column
df.drop(columns=['Unnamed: 0'], inplace=True)

# Rename columns for consistency
column_names = {
    'STATE/UT': 'State',
    'DISTRICT': 'District',
    'Year': 'Year',
    'Rape': 'Rape Cases',
    'Kidnapping and Abduction': 'Kidnap and Assault',
    'Dowry Deaths': 'Dowry Deaths',
    'Assault on women with intent to outrage her modesty': 'Assault on Women',
    'Insult to modesty of Women': 'Insult to Modesty',
    'Cruelty by Husband or his Relatives': 'Domestic Violence',
    'Importation of Girls': 'Women Trafficking'
}
df.rename(columns=column_names, inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns

# Group the data by crime category and sum the number of cases
crime_totals = df.iloc[:, 3:].sum()

# Generate colors for each bar
colors = sns.color_palette("husl", len(crime_totals))

# Create the bar plot
plt.figure(figsize=(12, 6))
crime_totals.plot(kind='bar', color=colors)
plt.title('Total Number of Cases per Crime Category')
plt.xlabel('Crime Category')
plt.ylabel('Number of Cases')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability
plt.tight_layout()
plt.show()

# Identify the highest crime category
highest_crime = crime_totals.idxmax()
print(f"The highest crime category is: {highest_crime}")

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Summing all crime categories for each district, state, and year
df['Total_Crimes'] = df.iloc[:, 3:].sum(axis=1)

# Standardize state names
df['State'] = df['State'].str.upper().str.strip()

# Group by state and year for analysis
grouped_states = df.groupby(['State', 'Year']).sum().reset_index()

# Pivot the data for heatmap
import seaborn as sns

heatmap_data = grouped_states.pivot_table(values='Total_Crimes', index='State', columns='Year', aggfunc='sum', fill_value=0)
plt.figure(figsize=(15, 10))
sns.heatmap(heatmap_data, cmap="YlGnBu", linecolor='white', linewidths=0.5)
plt.title('Heatmap of Total Crimes by State and Year')
plt.xlabel('Year')
plt.ylabel('State')
plt.show()

# Aggregate total crimes by state for bar graph
total_crimes_by_state = grouped_states.groupby('State')['Total_Crimes'].sum().reset_index()
plt.figure(figsize=(12, 8))
plt.bar(total_crimes_by_state['State'], total_crimes_by_state['Total_Crimes'], color='black')
plt.xlabel('State', fontsize=14)
plt.ylabel('Total Crimes', fontsize=14)
plt.title('Total Crimes by State', fontsize=16)
plt.xticks(rotation=90, fontsize=12)
plt.yticks(fontsize=12)
# Remove gridlines for a clean look
plt.grid(False)
plt.tight_layout()
plt.show()

# Create a pivot table with states as columns and years as rows
data = grouped_states[['Year', 'State', 'Total_Crimes']]
data_pivot = data.pivot(index='Year', columns='State', values='Total_Crimes').fillna(0)

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Scale the data
scaled_data = scaler.fit_transform(data_pivot)

from tensorflow.keras.preprocessing.sequence import pad_sequences # Import the pad_sequences function

# Prepare sequences using all available years for prediction
X, y = [], []
for i in range(1, len(scaled_data)):
    X.append(scaled_data[:i])  # Use all years up to the current year
    y.append(scaled_data[i])  # Predict the next year's values

# Prepare sequences using all available years for prediction
X, y = [], []
for i in range(1, len(scaled_data)):
    X.append(scaled_data[:i])  # Use all years up to the current year
    y.append(scaled_data[i])  # Predict the next year's values

# Pad sequences to uniform length
X_padded = pad_sequences(X, padding='pre', dtype='float32')
y = np.array(y)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout

# # Define the CNN-LSTM model with additional layers
# model = Sequential([
#     # Convolutional layers
#     Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
#     MaxPooling1D(pool_size=2),
#     Conv1D(filters=128, kernel_size=3, activation='relu'),  # Additional Conv1D layer
#     MaxPooling1D(pool_size=2),

#     # LSTM layers
#     LSTM(100, return_sequences=True),  # First LSTM layer
#     LSTM(50, return_sequences=False),  # Second LSTM layer

#     # Fully connected layers
#     Dense(128, activation='relu'),
#     Dropout(0.3),
#     Dense(64, activation='relu'),  # Additional Dense layer
#     Dense(y_train.shape[1], activation='linear')  # Linear activation for regression
# ])
# Define the CNN-LSTM model
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(pool_size=2),
    LSTM(100, return_sequences=False),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(y_train.shape[1], activation='linear')  # Linear activation for regression
])


# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=1)

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Make predictions with the model
y_pred_actual = scaler.inverse_transform(model.predict(X_test))

# Inverse scaling for the true values
y_test_actual = scaler.inverse_transform(y_test)

# Calculate the MAE (already done)
test_mae = np.mean(np.abs(y_test_actual - y_pred_actual))  # or use model.evaluate()

# Calculate the range of actual values
actual_min = np.min(y_test_actual)
actual_max = np.max(y_test_actual)
range_of_actual = actual_max - actual_min

# Check the values of MAE and range to debug
# print(f"Test MAE: {test_mae:.4f}")
# print(f"Range of Actual Values: {range_of_actual:.4f}")

# Calculate accuracy based on the range of actual values
accuracy = 100 * (1 - (test_mae / range_of_actual))

# Display accuracy
print(f"Accuracy: {accuracy:.2f}%")

# Predict on test data
y_pred = model.predict(X_test)

# Inverse scale predictions and actual values
y_test_inverse = scaler.inverse_transform(y_test)
y_pred_inverse = scaler.inverse_transform(y_pred)

# Visualize predictions
indices = np.arange(len(y_test))
plt.figure(figsize=(15, 5))
plt.bar(indices - 0.2, np.mean(y_test_inverse, axis=1), width=0.4, label='Actual')
plt.bar(indices + 0.2, np.mean(y_pred_inverse, axis=1), width=0.4, label='Predicted')
plt.title('Comparison of Actual and Predicted Crime Rates')
plt.xlabel('Samples')
plt.ylabel('Crime Rates')
plt.legend()
plt.show()

# Visualizing the comparison between predicted and actual crime rates
import matplotlib.pyplot as plt

# Predict on test data
y_pred = model.predict(X_test)

# Inverse scale predictions and actual values
y_test_inverse = scaler.inverse_transform(y_test)
y_pred_inverse = scaler.inverse_transform(y_pred)

# Plot actual vs predicted values
plt.figure(figsize=(10, 5))
plt.plot(np.mean(y_test_inverse, axis=1), label='Actual')
plt.plot(np.mean(y_pred_inverse, axis=1), label='Predicted')
plt.title('Actual vs Predicted Crime Rates')
plt.xlabel('Samples')
plt.ylabel('Crime Rates')
plt.legend()
plt.show()

# Get the most recent 13 years of data (last 13 years)
latest_years_data = scaled_data[-13:]  # Take the last 13 years of data

# Reshape the data to match the model's expected input shape (1, 13, 36)
latest_years_data = latest_years_data.reshape(1, 13, 36)  # (1, timesteps=13, features=36)

# Make prediction for the next year using the trained model
future_prediction = model.predict(latest_years_data)

# Inverse scale the predicted values to get the actual crime rates
future_prediction_inverse = scaler.inverse_transform(future_prediction)

# Create a DataFrame for the predicted crime rates
predicted_crimes_df = pd.DataFrame(future_prediction_inverse, columns=data_pivot.columns)

# Sort the states by the predicted crime rates in descending order
sorted_states = predicted_crimes_df.T.sort_values(by=0, ascending=False)

# Get the top 10 states with the highest predicted crime rates
top_10_states = sorted_states.head(10)

# Display the top 10 states
print("Top 10 States with the Highest Predicted Crimes in the Future:")
print(top_10_states)

# Visualize the predictions for the top 10 states
plt.figure(figsize=(10, 5))
top_10_states.plot(kind='bar',color='black', legend=False)
plt.title('Top 10 States with Highest Predicted Crimes')
plt.ylabel('Predicted Crime Rate')
plt.xlabel('State')
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout

# Define the CNN-only model
cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(y_train.shape[1], activation='linear')
])

# Compile the CNN model
cnn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the CNN model
cnn_history = cnn_model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=1)

# Evaluate the CNN model
y_pred_cnn = cnn_model.predict(X_test)
y_pred_cnn_actual = scaler.inverse_transform(y_pred_cnn)
y_test_actual = scaler.inverse_transform(y_test)
test_mae_cnn = np.mean(np.abs(y_test_actual - y_pred_cnn_actual))
accuracy_cnn = 100 * (1 - (test_mae_cnn / range_of_actual))
print(f"CNN Model Accuracy: {accuracy_cnn:.2f}%")

# Define the LSTM-only model
lstm_model = Sequential([
    LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(50, return_sequences=False),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(y_train.shape[1], activation='linear')
])

# Compile the LSTM model
lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the LSTM model
lstm_history = lstm_model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=1)

# Evaluate the LSTM model
y_pred_lstm = lstm_model.predict(X_test)
y_pred_lstm_actual = scaler.inverse_transform(y_pred_lstm)
test_mae_lstm = np.mean(np.abs(y_test_actual - y_pred_lstm_actual))
accuracy_lstm = 100 * (1 - (test_mae_lstm / range_of_actual))
print(f"LSTM Model Accuracy: {accuracy_lstm:.2f}%")

# Print and compare accuracies
print("\nModel Comparison:")
print(f"CNN Model Accuracy: {accuracy_cnn:.2f}%")
print(f"LSTM Model Accuracy: {accuracy_lstm:.2f}%")
print(f"CNN-LSTM Model Accuracy: {accuracy:.2f}%")

import matplotlib.pyplot as plt

# Assume `predicted_crimes_df` contains the predicted crime rates for all states

# Visualize the predictions for all states in a simple bar graph without sorting
plt.figure(figsize=(12, 8))  # Adjust the figure size for a spacious layout

# Plot the bars with black color
plt.bar(predicted_crimes_df.columns, predicted_crimes_df.iloc[0], color='black')

# Add title and labels with larger font for clarity
plt.title('Predicted Crimes by State for the Future', fontsize=16)
plt.ylabel('Predicted Crime Rate', fontsize=14)
plt.xlabel('State', fontsize=14)

# Rotate x-axis labels for better readability
plt.xticks(rotation=90, fontsize=12)

# Adjust y-axis tick size for better readability
plt.yticks(fontsize=12)

# Remove gridlines for a clean look
plt.grid(False)

# Add spacing between bars for a more spacious look
plt.tight_layout()

# Show the plot
plt.show()

